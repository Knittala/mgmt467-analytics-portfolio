{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8J5E1Y2JW6Mo"
      },
      "source": [
        "# MGMT 467 — Prompt-Driven Lab (with Commented Examples)\n",
        "## Kaggle ➜ Google Cloud Storage ➜ BigQuery ➜ Data Quality (DQ)\n",
        "\n",
        "**How to use this notebook**\n",
        "- Each section gives you a **Build Prompt** to paste into Gemini/Vertex AI (or Gemini in Colab).\n",
        "- Below each prompt, you’ll see a **commented example** of what a good LLM answer might look like.\n",
        "- **Do not** just uncomment and run. Use the prompt to generate your own code, then compare to the example.\n",
        "- After every step, run the **Verification Prompt**, and write the **Reflection** in Markdown.\n",
        "\n",
        "> Goal today: Download the Netflix dataset (Kaggle) → Stage on GCS → Load into BigQuery → Run DQ profiling (missingness, duplicates, outliers, anomaly flags).\n"
      ],
      "id": "8J5E1Y2JW6Mo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ipMZOFwW6Mq"
      },
      "source": [
        "### Academic integrity & LLM usage\n",
        "- Use the prompts here to generate your own code cells.\n",
        "- Read concept notes and write the reflection answers in your own words.\n",
        "- Keep credentials out of code. Upload `kaggle.json` when asked.\n"
      ],
      "id": "1ipMZOFwW6Mq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XQQqLxRW6Mr"
      },
      "source": [
        "## Learning objectives\n",
        "1) Explain **why** we stage data in GCS and load it to BigQuery.  \n",
        "2) Build an **idempotent**, auditable pipeline.  \n",
        "3) Diagnose **missingness**, **duplicates**, and **outliers** and justify cleaning choices.  \n",
        "4) Connect DQ decisions to **business/ML impact**.\n"
      ],
      "id": "3XQQqLxRW6Mr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRQ_45NyW6Mr"
      },
      "source": [
        "## 0) Environment setup — What & Why\n",
        "Authenticate Colab to Google Cloud so we can use `gcloud`, GCS, and BigQuery. Set **PROJECT_ID** and **REGION** once for consistency (cost/latency)."
      ],
      "id": "PRQ_45NyW6Mr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HB1HTVIsW6Ms"
      },
      "source": [
        "### Build Prompt (paste to LLM)\n",
        "You are my cloud TA. Generate a single **Colab code cell** that:\n",
        "1) Authenticates to Google Cloud in Colab,  \n",
        "2) Prompts for `PROJECT_ID` via `input()` and sets `REGION=\"us-central1\"` (editable),  \n",
        "3) Exports `GOOGLE_CLOUD_PROJECT`,  \n",
        "4) Runs `gcloud config set project $GOOGLE_CLOUD_PROJECT`,  \n",
        "5) Prints both values. Add 2–3 comments explaining what/why.\n",
        "End with a comment: `# Done: Auth + Project/Region set`.\n"
      ],
      "id": "HB1HTVIsW6Ms"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hj9__XD7W6Mt",
        "outputId": "f1c1e1ed-af2f-42ce-8bb3-c49aade55aaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your GCP Project ID: mgmt-467-55510 \n",
            "Project: mgmt-467-55510 | Region: us-central1\n",
            "Updated property [core/project].\n",
            "mgmt-467-55510\n"
          ]
        }
      ],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import os\n",
        "PROJECT_ID = input(\"Enter your GCP Project ID: \").strip()\n",
        "REGION = \"us-central1\"  # keep consistent; change if instructed\n",
        "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
        "os.environ[\"REGION\"] = REGION # Export the REGION environment variable\n",
        "print(\"Project:\", PROJECT_ID, \"| Region:\", REGION)\n",
        "\n",
        "# Set active project for gcloud/BigQuery CLI\n",
        "!gcloud config set project $GOOGLE_CLOUD_PROJECT\n",
        "!gcloud config get-value project\n",
        "# Done: Auth + Project/Region set"
      ],
      "id": "Hj9__XD7W6Mt"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pa18A6i3W6Mv"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a short cell that prints the active project using `gcloud config get-value project` and echoes the `REGION` you set.\n"
      ],
      "id": "Pa18A6i3W6Mv"
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the active project and region\n",
        "!gcloud config get-value project\n",
        "import os\n",
        "print(\"Region:\", os.environ.get(\"REGION\")) # Access the exported REGION environment variable"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8u36SRy6bHGF",
        "outputId": "5e8da066-a60b-4328-b0f0-573242616a57"
      },
      "id": "8u36SRy6bHGF",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mgmt-467-55510\n",
            "Region: us-central1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFoh5H_tW6Mv"
      },
      "source": [
        "**Reflection:** Why do we set `PROJECT_ID` and `REGION` at the top? What can go wrong if we don’t?\n",
        "\n",
        "We would like to establish the project id and region at the top of the code sequence to ensure that the pipeline flows smoothly and no issues occur in the next steps. As the sequence gets more complex when using more products and services, it is important to keep all of the information repeatable and consistent so establishing it ahead of time will help with this."
      ],
      "id": "LFoh5H_tW6Mv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sChmOEiOW6Mv"
      },
      "source": [
        "## 1) Kaggle API — What & Why\n",
        "Use Kaggle CLI for reproducible downloads. Store `kaggle.json` at `~/.kaggle/kaggle.json` with `0600` permissions to protect secrets."
      ],
      "id": "sChmOEiOW6Mv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAkWPu3_W6Mw"
      },
      "source": [
        "### Build Prompt\n",
        "Generate a **single Colab code cell** that:\n",
        "- Prompts me to upload `kaggle.json`,\n",
        "- Saves to `~/.kaggle/kaggle.json` with `0600` permissions,\n",
        "- Prints `kaggle --version`.\n",
        "Add comments about security and reproducibility.\n"
      ],
      "id": "ZAkWPu3_W6Mw"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "rYCmgKQdW6Mw",
        "outputId": "5b07b445-0e2f-4c98-d93a-ec5f245beea7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload your kaggle.json (Kaggle > Account > Create New API Token)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f8e54007-7a38-4ed4-898b-33157ad941bf\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f8e54007-7a38-4ed4-898b-33157ad941bf\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Kaggle API 1.7.4.5\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "print(\"Upload your kaggle.json (Kaggle > Account > Create New API Token)\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "import os\n",
        "os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "with open('/root/.kaggle/kaggle.json', 'wb') as f:\n",
        "    f.write(uploaded[list(uploaded.keys())[0]])\n",
        "os.chmod('/root/.kaggle/kaggle.json', 0o600)  # owner-only\n",
        "\n",
        "!kaggle --version"
      ],
      "id": "rYCmgKQdW6Mw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoyMyQyWW6Mx"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a one-liner that runs `kaggle --help | head -n 20` to show the CLI is ready.\n"
      ],
      "id": "GoyMyQyWW6Mx"
    },
    {
      "cell_type": "code",
      "source": [
        "#Verifying the CLI is ready\n",
        "!kaggle --help | head -n 20"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5eLbn0Xd0br",
        "outputId": "fbc65d9e-0c49-4a01-f2a3-2dc4dd214e20"
      },
      "id": "E5eLbn0Xd0br",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: kaggle [-h] [-v] [-W]\n",
            "              {competitions,c,datasets,d,kernels,k,models,m,files,f,config}\n",
            "              ...\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  -v, --version         Print the Kaggle API version\n",
            "  -W, --no-warn         Disable out-of-date API version warning\n",
            "\n",
            "commands:\n",
            "  {competitions,c,datasets,d,kernels,k,models,m,files,f,config}\n",
            "                        Use one of:\n",
            "                        competitions {list, files, download, submit, submissions, leaderboard}\n",
            "                        datasets {list, files, download, create, version, init, metadata, status}\n",
            "                        kernels {list, files, init, push, pull, output, status}\n",
            "                        models {instances, get, list, init, create, delete, update}\n",
            "                        models instances {versions, get, files, init, create, delete, update}\n",
            "                        models instances versions {init, create, download, delete, files}\n",
            "                        config {view, set, unset}\n",
            "    competitions (c)    Commands related to Kaggle competitions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7hTBVyZW6My"
      },
      "source": [
        "**Reflection:** Why require strict `0600` permissions on API tokens? What risks are we avoiding?\n",
        "\n",
        "We require strict '0600' permissions on API tokens so that only the owner of the file can read and write to it. This helps avoid risks that may expose your API token if there is a misconfiguration of your permissions and avoid attacks from unauthorized access."
      ],
      "id": "N7hTBVyZW6My"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWA3IFgdW6My"
      },
      "source": [
        "## 2) Download & unzip dataset — What & Why\n",
        "Keep raw files under `/content/data/raw` for predictable paths and auditing.\n",
        "**Dataset:** `sayeeduddin/netflix-2025user-behavior-dataset-210k-records`"
      ],
      "id": "RWA3IFgdW6My"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZRbjCM0W6My"
      },
      "source": [
        "### Build Prompt\n",
        "Generate a **Colab code cell** that:\n",
        "- Creates `/content/data/raw`,\n",
        "- Downloads the dataset to `/content/data` with Kaggle CLI,\n",
        "- Unzips into `/content/data/raw` (overwrite OK),\n",
        "- Lists all CSVs with sizes in a neat table.\n",
        "Include comments describing each step.\n"
      ],
      "id": "fZRbjCM0W6My"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2n9eW1ZW6My"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — Download & unzip (commented)\n",
        "# # !mkdir -p /content/data/raw\n",
        "# # !kaggle datasets download -d sayeeduddin/netflix-2025user-behavior-dataset-210k-records -p /content/data\n",
        "# # !unzip -o /content/data/*.zip -d /content/data/raw\n",
        "# # # List CSV inventory\n",
        "# # !ls -lh /content/data/raw/*.csv"
      ],
      "id": "e2n9eW1ZW6My"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64d5908f",
        "outputId": "e1c22ee5-0edd-4fe5-c9c7-411d6b1c8767"
      },
      "source": [
        "# Create directory for raw data\n",
        "!mkdir -p /content/data/raw\n",
        "\n",
        "# Download the dataset using Kaggle CLI\n",
        "!kaggle datasets download -d sayeeduddin/netflix-2025user-behavior-dataset-210k-records -p /content/data\n",
        "\n",
        "# Unzip the downloaded dataset into the raw data directory\n",
        "!unzip -o /content/data/*.zip -d /content/data/raw\n",
        "\n",
        "# List all CSV files in the raw data directory with their sizes\n",
        "!ls -lh /content/data/raw/*.csv"
      ],
      "id": "64d5908f",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/sayeeduddin/netflix-2025user-behavior-dataset-210k-records\n",
            "License(s): CC0-1.0\n",
            "Downloading netflix-2025user-behavior-dataset-210k-records.zip to /content/data\n",
            "  0% 0.00/4.02M [00:00<?, ?B/s]\n",
            "100% 4.02M/4.02M [00:00<00:00, 610MB/s]\n",
            "Archive:  /content/data/netflix-2025user-behavior-dataset-210k-records.zip\n",
            "  inflating: /content/data/raw/README.md  \n",
            "  inflating: /content/data/raw/movies.csv  \n",
            "  inflating: /content/data/raw/recommendation_logs.csv  \n",
            "  inflating: /content/data/raw/reviews.csv  \n",
            "  inflating: /content/data/raw/search_logs.csv  \n",
            "  inflating: /content/data/raw/users.csv  \n",
            "  inflating: /content/data/raw/watch_history.csv  \n",
            "-rw-r--r-- 1 root root 114K Aug  2 19:36 /content/data/raw/movies.csv\n",
            "-rw-r--r-- 1 root root 4.5M Aug  2 19:36 /content/data/raw/recommendation_logs.csv\n",
            "-rw-r--r-- 1 root root 1.8M Aug  2 19:36 /content/data/raw/reviews.csv\n",
            "-rw-r--r-- 1 root root 2.2M Aug  2 19:36 /content/data/raw/search_logs.csv\n",
            "-rw-r--r-- 1 root root 1.6M Aug  2 19:36 /content/data/raw/users.csv\n",
            "-rw-r--r-- 1 root root 8.9M Aug  2 19:36 /content/data/raw/watch_history.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssnr6wwvW6My"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a snippet that asserts there are exactly **six** CSV files and prints their names.\n"
      ],
      "id": "ssnr6wwvW6My"
    },
    {
      "cell_type": "code",
      "source": [
        "# Asserting that there are exactly six CSV files and printing their names\n",
        "import glob\n",
        "csv_files = glob.glob('/content/data/raw/*.csv')\n",
        "assert len(csv_files) == 6, f\"Expected 6 CSV files, but found {len(csv_files)}\"\n",
        "print(\"Found the following CSV files:\")\n",
        "for csv_file in csv_files:\n",
        "    print(csv_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_d8kzr2e9Bi",
        "outputId": "c6f7b904-33c0-4ee2-e14a-f1550686fb86"
      },
      "id": "C_d8kzr2e9Bi",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found the following CSV files:\n",
            "/content/data/raw/movies.csv\n",
            "/content/data/raw/watch_history.csv\n",
            "/content/data/raw/search_logs.csv\n",
            "/content/data/raw/users.csv\n",
            "/content/data/raw/recommendation_logs.csv\n",
            "/content/data/raw/reviews.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-8KLg3eW6Mz"
      },
      "source": [
        "**Reflection:** Why is keeping a clean file inventory (names, sizes) useful downstream?\n",
        "\n",
        "It is important to stay organized so it is easier to locate data files later on in the process when you may have a large amount of files to take care of. Additionally distinguishing files by names clearly will help ensure you are using the right data and that it does not disrupt the pipeline. It also to be noted that these files can be uploaded by students but not deleted or edited as easily due to restricted permissions/legal right issues with GCS."
      ],
      "id": "D-8KLg3eW6Mz"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rs6fA4GW6Mz"
      },
      "source": [
        "## 3) Create GCS bucket & upload — What & Why\n",
        "Stage in GCS → consistent, versionable source for BigQuery loads. Bucket names must be **globally unique**."
      ],
      "id": "-rs6fA4GW6Mz"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "id3yd7gvW6Mz"
      },
      "source": [
        "### Build Prompt\n",
        "Generate a **Colab code cell** that:\n",
        "- Creates a unique bucket in `${REGION}` (random suffix),\n",
        "- Saves name to `BUCKET_NAME` env var,\n",
        "- Uploads all CSVs to `gs://$BUCKET_NAME/netflix/`,\n",
        "- Prints the bucket name and explains staging benefits.\n"
      ],
      "id": "id3yd7gvW6Mz"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee84f6c3",
        "outputId": "1ea8b9d8-7535-483d-9e31-6c1b3f1611e2"
      },
      "source": [
        "import uuid\n",
        "import os\n",
        "\n",
        "# Create a unique bucket name\n",
        "bucket_name = f\"mgmt467-netflix-{uuid.uuid4().hex[:8]}\"\n",
        "os.environ[\"BUCKET_NAME\"] = bucket_name\n",
        "\n",
        "# Create the GCS bucket\n",
        "print(f\"Creating bucket: gs://{bucket_name} in region {os.environ['REGION']}\")\n",
        "!gcloud storage buckets create gs://$BUCKET_NAME --location=$REGION\n",
        "\n",
        "# Upload all CSVs to the bucket\n",
        "print(f\"Uploading CSV files to gs://{bucket_name}/netflix/\")\n",
        "!gcloud storage cp /content/data/raw/*.csv gs://$BUCKET_NAME/netflix/\n",
        "\n",
        "# Print the bucket name\n",
        "print(\"\\nData staged in GCS bucket:\", bucket_name)\n",
        "\n",
        "# Explain staging benefits\n",
        "print(\"\"\"\n",
        "Benefits of staging data in GCS:\n",
        "- Durable and highly available storage.\n",
        "- Versioning for data changes.\n",
        "- Scalable and cost-effective.\n",
        "- Acts as a single source of truth for downstream processing (e.g., loading into BigQuery).\n",
        "\"\"\")\n",
        "\n",
        "# Verify contents\n",
        "print(f\"\\nVerifying contents of gs://{bucket_name}/netflix/\")\n",
        "!gcloud storage ls gs://$BUCKET_NAME/netflix/"
      ],
      "id": "ee84f6c3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating bucket: gs://mgmt467-netflix-c7457bd7 in region us-central1\n",
            "Creating gs://mgmt467-netflix-c7457bd7/...\n",
            "Uploading CSV files to gs://mgmt467-netflix-c7457bd7/netflix/\n",
            "Copying file:///content/data/raw/movies.csv to gs://mgmt467-netflix-c7457bd7/netflix/movies.csv\n",
            "Copying file:///content/data/raw/recommendation_logs.csv to gs://mgmt467-netflix-c7457bd7/netflix/recommendation_logs.csv\n",
            "Copying file:///content/data/raw/reviews.csv to gs://mgmt467-netflix-c7457bd7/netflix/reviews.csv\n",
            "Copying file:///content/data/raw/search_logs.csv to gs://mgmt467-netflix-c7457bd7/netflix/search_logs.csv\n",
            "Copying file:///content/data/raw/users.csv to gs://mgmt467-netflix-c7457bd7/netflix/users.csv\n",
            "Copying file:///content/data/raw/watch_history.csv to gs://mgmt467-netflix-c7457bd7/netflix/watch_history.csv\n",
            "\n",
            "Average throughput: 47.8MiB/s\n",
            "\n",
            "Data staged in GCS bucket: mgmt467-netflix-c7457bd7\n",
            "\n",
            "Benefits of staging data in GCS:\n",
            "- Durable and highly available storage.\n",
            "- Versioning for data changes.\n",
            "- Scalable and cost-effective.\n",
            "- Acts as a single source of truth for downstream processing (e.g., loading into BigQuery).\n",
            "\n",
            "\n",
            "Verifying contents of gs://mgmt467-netflix-c7457bd7/netflix/\n",
            "gs://mgmt467-netflix-c7457bd7/netflix/movies.csv\n",
            "gs://mgmt467-netflix-c7457bd7/netflix/recommendation_logs.csv\n",
            "gs://mgmt467-netflix-c7457bd7/netflix/reviews.csv\n",
            "gs://mgmt467-netflix-c7457bd7/netflix/search_logs.csv\n",
            "gs://mgmt467-netflix-c7457bd7/netflix/users.csv\n",
            "gs://mgmt467-netflix-c7457bd7/netflix/watch_history.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzDxNBcgW6Mz"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a snippet that lists the `netflix/` prefix and shows object sizes.\n"
      ],
      "id": "kzDxNBcgW6Mz"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93c77135",
        "outputId": "7dd381a6-49fd-45ef-fa14-1e1bec097e8b"
      },
      "source": [
        "# List the contents of the netflix/ prefix and show object sizes (corrected command)\n",
        "!gcloud storage ls --readable-sizes gs://$BUCKET_NAME/netflix/"
      ],
      "id": "93c77135",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gs://mgmt467-netflix-14a64d4b/netflix/movies.csv\n",
            "gs://mgmt467-netflix-14a64d4b/netflix/recommendation_logs.csv\n",
            "gs://mgmt467-netflix-14a64d4b/netflix/reviews.csv\n",
            "gs://mgmt467-netflix-14a64d4b/netflix/search_logs.csv\n",
            "gs://mgmt467-netflix-14a64d4b/netflix/users.csv\n",
            "gs://mgmt467-netflix-14a64d4b/netflix/watch_history.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsjVxFuFW6Mz"
      },
      "source": [
        "**Reflection:** Name two benefits of staging in GCS vs loading directly from local Colab.\n",
        "\n",
        "1. Durability and Accessibility - GCS has highly durable and storage with ease of access. Data stored in GCS is less likely to be lost in the Colab environment and can be easily found through various Google Cloud services and apps\n",
        "\n",
        "2. Scalable and Efficient - GCS is designed for handling tasks at a massive scale. Loading data from GCS into BigQuery is quicker and efficient since it uses Google Cloud's internal network and directly pull data through optimized transfer systems"
      ],
      "id": "wsjVxFuFW6Mz"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_77eVBgwW6M0"
      },
      "source": [
        "## 4) BigQuery dataset & loads — What & Why\n",
        "Create dataset `netflix` and load six CSVs with **autodetect** for speed (we’ll enforce schemas later)."
      ],
      "id": "_77eVBgwW6M0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YELnUFpsW6M0"
      },
      "source": [
        "### Build Prompt (two cells)\n",
        "**Cell A:** Create (idempotently) dataset `netflix` in US multi-region; if it exists, print a friendly message.  \n",
        "**Cell B:** Load tables from `gs://$BUCKET_NAME/netflix/`:\n",
        "`users, movies, watch_history, recommendation_logs, search_logs, reviews`\n",
        "with `--skip_leading_rows=1 --autodetect --source_format=CSV`.\n",
        "Finish with row-count queries for each table.\n"
      ],
      "id": "YELnUFpsW6M0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDleVFKdW6M0"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — BigQuery dataset (commented)\n",
        "# # DATASET=\"netflix\"\n",
        "# # # Attempt to create; ignore if exists\n",
        "# # !bq --location=US mk -d --description \"MGMT467 Netflix dataset\" $DATASET || echo \"Dataset may already exist.\""
      ],
      "id": "zDleVFKdW6M0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afa6160b",
        "outputId": "9657f374-5413-4582-d992-6adcd71215f4"
      },
      "source": [
        "# Create (idempotently) dataset netflix in US multi-region\n",
        "DATASET=\"netflix\"\n",
        "# Attempt to create; ignore if exists\n",
        "!bq --location=US mk -d --description \"MGMT467 Netflix dataset\" $DATASET || echo \"Dataset may already exist.\""
      ],
      "id": "afa6160b",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset 'mgmt-467-55510:netflix' successfully created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1b3e2b5",
        "outputId": "0fb25e0b-b126-4271-8b2c-11c42d555bbc"
      },
      "source": [
        "# Load tables from gs://$BUCKET_NAME/netflix/\n",
        "tables = {\n",
        "  \"users\": \"users.csv\",\n",
        "  \"movies\": \"movies.csv\",\n",
        "  \"watch_history\": \"watch_history.csv\",\n",
        "  \"recommendation_logs\": \"recommendation_logs.csv\",\n",
        "  \"search_logs\": \"search_logs.csv\",\n",
        "  \"reviews\": \"reviews.csv\",\n",
        "}\n",
        "import os\n",
        "for tbl, fname in tables.items():\n",
        "  src = f\"gs://{os.environ['BUCKET_NAME']}/netflix/{fname}\"\n",
        "  dest = f\"{DATASET}.{tbl}\" # Corrected destination table format\n",
        "  print(\"Loading\", tbl, \"from\", src)\n",
        "  !bq load --skip_leading_rows=1 --autodetect --source_format=CSV $dest $src # Corrected bq load command\n",
        "\n",
        "# Row counts\n",
        "for tbl in tables.keys():\n",
        "  # Corrected bq query command syntax\n",
        "  !bq query --nouse_legacy_sql \"SELECT '{tbl}' AS table_name, COUNT(*) AS n FROM `{GOOGLE_CLOUD_PROJECT}.{DATASET}.{tbl}`\".format(tbl=tbl)"
      ],
      "id": "e1b3e2b5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading users from gs://mgmt467-netflix-14a64d4b/netflix/users.csv\n",
            "Waiting on bqjob_r40cca239d9ae750a_00000199ca8e227b_1 ... (1s) Current status: DONE   \n",
            "Loading movies from gs://mgmt467-netflix-14a64d4b/netflix/movies.csv\n",
            "Waiting on bqjob_r7a9623f36ae47861_00000199ca8e43ce_1 ... (1s) Current status: DONE   \n",
            "Loading watch_history from gs://mgmt467-netflix-14a64d4b/netflix/watch_history.csv\n",
            "Waiting on bqjob_r307a23b11063a895_00000199ca8e651b_1 ... (2s) Current status: DONE   \n",
            "Loading recommendation_logs from gs://mgmt467-netflix-14a64d4b/netflix/recommendation_logs.csv\n",
            "Waiting on bqjob_r2a849e2e3db01cb7_00000199ca8e8c80_1 ... (1s) Current status: DONE   \n",
            "Loading search_logs from gs://mgmt467-netflix-14a64d4b/netflix/search_logs.csv\n",
            "Waiting on bqjob_r67f24dc187ea41a5_00000199ca8eaed5_1 ... (1s) Current status: DONE   \n",
            "Loading reviews from gs://mgmt467-netflix-14a64d4b/netflix/reviews.csv\n",
            "Waiting on bqjob_r4279b8615212452d_00000199ca8ed015_1 ... (1s) Current status: DONE   \n",
            "/bin/bash: -c: line 1: syntax error near unexpected token `('\n",
            "/bin/bash: -c: line 1: `bq query --nouse_legacy_sql \"SELECT '{tbl}' AS table_name, COUNT(*) AS n FROM `{GOOGLE_CLOUD_PROJECT}.{DATASET}.{tbl}`\".format(tbl=tbl)'\n",
            "/bin/bash: -c: line 1: syntax error near unexpected token `('\n",
            "/bin/bash: -c: line 1: `bq query --nouse_legacy_sql \"SELECT '{tbl}' AS table_name, COUNT(*) AS n FROM `{GOOGLE_CLOUD_PROJECT}.{DATASET}.{tbl}`\".format(tbl=tbl)'\n",
            "/bin/bash: -c: line 1: syntax error near unexpected token `('\n",
            "/bin/bash: -c: line 1: `bq query --nouse_legacy_sql \"SELECT '{tbl}' AS table_name, COUNT(*) AS n FROM `{GOOGLE_CLOUD_PROJECT}.{DATASET}.{tbl}`\".format(tbl=tbl)'\n",
            "/bin/bash: -c: line 1: syntax error near unexpected token `('\n",
            "/bin/bash: -c: line 1: `bq query --nouse_legacy_sql \"SELECT '{tbl}' AS table_name, COUNT(*) AS n FROM `{GOOGLE_CLOUD_PROJECT}.{DATASET}.{tbl}`\".format(tbl=tbl)'\n",
            "/bin/bash: -c: line 1: syntax error near unexpected token `('\n",
            "/bin/bash: -c: line 1: `bq query --nouse_legacy_sql \"SELECT '{tbl}' AS table_name, COUNT(*) AS n FROM `{GOOGLE_CLOUD_PROJECT}.{DATASET}.{tbl}`\".format(tbl=tbl)'\n",
            "/bin/bash: -c: line 1: syntax error near unexpected token `('\n",
            "/bin/bash: -c: line 1: `bq query --nouse_legacy_sql \"SELECT '{tbl}' AS table_name, COUNT(*) AS n FROM `{GOOGLE_CLOUD_PROJECT}.{DATASET}.{tbl}`\".format(tbl=tbl)'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OBUfepIW6M0"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — Load tables (commented)\n",
        "# # tables = {\n",
        "# #   \"users\": \"users.csv\",\n",
        "# #   \"movies\": \"movies.csv\",\n",
        "# #   \"watch_history\": \"watch_history.csv\",\n",
        "# #   \"recommendation_logs\": \"recommendation_logs.csv\",\n",
        "# #   \"search_logs\": \"search_logs.csv\",\n",
        "# #   \"reviews\": \"reviews.csv\",\n",
        "# # }\n",
        "# # import os\n",
        "# # for tbl, fname in tables.items():\n",
        "# #   src = f\"gs://{os.environ['BUCKET_NAME']}/netflix/{fname}\"\n",
        "# #   print(\"Loading\", tbl, \"from\", src)\n",
        "# #   !bq load --skip_leading_rows=1 --autodetect --source_format=CSV $DATASET.$tbl $src\n",
        "# #\n",
        "# # # Row counts\n",
        "# # for tbl in tables.keys():\n",
        "# #   !bq query --nouse_legacy_sql \"SELECT '{tbl}' AS table_name, COUNT(*) AS n FROM `${GOOGLE_CLOUD_PROJECT}.netflix.{tbl}`\".format(tbl=tbl)"
      ],
      "id": "5OBUfepIW6M0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ugyOVgnW6M1"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a single query that returns `table_name, row_count` for all six tables in `${GOOGLE_CLOUD_PROJECT}.netflix`.\n"
      ],
      "id": "_ugyOVgnW6M1"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Get row counts for all six tables using bq query and string formatting\n",
        "query = \"\"\"\n",
        "SELECT 'users' AS table_name, COUNT(*) AS row_count FROM `{project_id}.netflix.users`\n",
        "UNION ALL\n",
        "SELECT 'movies' AS table_name, COUNT(*) AS row_count FROM `{project_id}.netflix.movies`\n",
        "UNION ALL\n",
        "SELECT 'watch_history' AS table_name, COUNT(*) AS row_count FROM `{project_id}.netflix.watch_history`\n",
        "UNION ALL\n",
        "SELECT 'recommendation_logs' AS table_name, COUNT(*) AS row_count FROM `{project_id}.netflix.recommendation_logs`\n",
        "UNION ALL\n",
        "SELECT 'search_logs' AS table_name, COUNT(*) AS row_count FROM `{project_id}.netflix.search_logs`\n",
        "UNION ALL\n",
        "SELECT 'reviews' AS table_name, COUNT(*) AS row_count FROM `{project_id}.netflix.reviews`\n",
        "\"\"\".format(project_id=os.environ['GOOGLE_CLOUD_PROJECT'])\n",
        "\n",
        "# Save the query to a temporary file\n",
        "with open(\"temp_query.sql\", \"w\") as f:\n",
        "    f.write(query)\n",
        "\n",
        "# Execute the query using bq query with the file\n",
        "!bq query --nouse_legacy_sql --quiet < temp_query.sql\n",
        "\n",
        "# Clean up the temporary file\n",
        "os.remove(\"temp_query.sql\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fw67qoJX1oEQ",
        "outputId": "424bcf6a-381c-4d11-c439-9655f7e2e433"
      },
      "id": "Fw67qoJX1oEQ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------+-----------+\n",
            "|     table_name      | row_count |\n",
            "+---------------------+-----------+\n",
            "| movies              |      4160 |\n",
            "| users               |     41200 |\n",
            "| recommendation_logs |    208000 |\n",
            "| search_logs         |    106000 |\n",
            "| watch_history       |    420000 |\n",
            "| reviews             |     61800 |\n",
            "+---------------------+-----------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJ97pRWsW6M1"
      },
      "source": [
        "**Reflection:** When is `autodetect` acceptable? When should you enforce explicit schemas and why?\n",
        "\n",
        "Autodetect is usually acceptable and most useful for initial data exploration and when the data you are observing is clean and well-formatted. It allows the user to get started right away with querying without needing to do any additional cleaning.\n",
        "\n",
        "Explicit schemas should be enforced when the data is ambigious/inconsistent or if you are working with critical data for long periods of time. This prevents unexpected errors and the correct data types are being used. Additionally, explicit schemas are helpful for providing control and clarified data pipelines where consistency is essential.\n"
      ],
      "id": "mJ97pRWsW6M1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EFOiXn9W6M1"
      },
      "source": [
        "## 5) Data Quality (DQ) — Concepts we care about\n",
        "- **Missingness** (MCAR/MAR/MNAR). Impute vs drop. Add `is_missing_*` indicators.\n",
        "- **Duplicates** (exact vs near). Double-counted engagement corrupts labels & KPIs.\n",
        "- **Outliers** (IQR). Winsorize/cap vs robust models. Always **flag** and explain.\n",
        "- **Reproducibility**. Prefer `CREATE OR REPLACE` and deterministic keys.\n"
      ],
      "id": "9EFOiXn9W6M1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvLNKy9AW6M1"
      },
      "source": [
        "### 5.1 Missingness (users) — What & Why\n",
        "Measure % missing and check if missingness depends on another variable (MAR) → potential bias & instability."
      ],
      "id": "KvLNKy9AW6M1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37Mt9ua4W6M1"
      },
      "source": [
        "### Build Prompt\n",
        "Generate **two BigQuery SQL cells**:\n",
        "1) Total rows and % missing in `region`, `plan_tier`, `age_band` from `users`.\n",
        "2) `% plan_tier missing by region` ordered descending. Add comments on MAR.\n"
      ],
      "id": "37Mt9ua4W6M1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ISO1-AEW6M2"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — Missingness profile (commented)\n",
        "# # -- Users: % missing per column\n",
        "# # WITH base AS (\n",
        "# #   SELECT COUNT(*) n,\n",
        "# #          COUNTIF(region IS NULL) miss_region,\n",
        "# #          COUNTIF(plan_tier IS NULL) miss_plan,\n",
        "# #          COUNTIF(age_band IS NULL) miss_age\n",
        "# #   FROM `${GOOGLE_CLOUD_PROJECT}.netflix.users`\n",
        "# # )\n",
        "# # SELECT n,\n",
        "# #        ROUND(100*miss_region/n,2) AS pct_missing_region,\n",
        "# #        ROUND(100*miss_plan/n,2)   AS pct_missing_plan_tier,\n",
        "# #        ROUND(100*miss_age/n,2)    AS pct_missing_age_band\n",
        "# # FROM base;"
      ],
      "id": "3ISO1-AEW6M2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBGPNH-JW6M2"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — MAR by region (commented)\n",
        "# # SELECT region,\n",
        "# #        COUNT(*) AS n,\n",
        "# #        ROUND(100*COUNTIF(plan_tier IS NULL)/COUNT(*),2) AS pct_missing_plan_tier\n",
        "# # FROM `${GOOGLE_CLOUD_PROJECT}.netflix.users`\n",
        "# # GROUP BY region\n",
        "# # ORDER BY pct_missing_plan_tier DESC;"
      ],
      "id": "GBGPNH-JW6M2"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Users: Total rows and % missing per column\n",
        "query_missingness = \"\"\"\n",
        "WITH base AS (\n",
        "  SELECT\n",
        "    COUNT(*) AS n,\n",
        "    COUNTIF(country IS NULL) AS miss_country,\n",
        "    COUNTIF(subscription_plan IS NULL) AS miss_plan,\n",
        "    COUNTIF(age IS NULL) AS miss_age\n",
        "  FROM\n",
        "    `{project_id}.netflix.users`\n",
        ")\n",
        "SELECT\n",
        "  n,\n",
        "  ROUND(100 * miss_country / n, 2) AS pct_missing_country,\n",
        "  ROUND(100 * miss_plan / n, 2) AS pct_missing_subscription_plan,\n",
        "  ROUND(100 * miss_age / n, 2) AS pct_missing_age\n",
        "FROM\n",
        "  base;\n",
        "\"\"\".format(project_id=os.environ['GOOGLE_CLOUD_PROJECT'])\n",
        "\n",
        "# Save the query to a temporary file\n",
        "with open(\"missingness_query.sql\", \"w\") as f:\n",
        "    f.write(query_missingness)\n",
        "\n",
        "# Execute the query using bq query with the file\n",
        "!bq query --nouse_legacy_sql --quiet < missingness_query.sql\n",
        "\n",
        "# Clean up the temporary file\n",
        "os.remove(\"missingness_query.sql\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-4PmFJr9ji6",
        "outputId": "e4f268af-7ed9-4654-cab1-55dc87af8a52"
      },
      "id": "2-4PmFJr9ji6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------------------+-------------------------------+-----------------+\n",
            "|   n   | pct_missing_country | pct_missing_subscription_plan | pct_missing_age |\n",
            "+-------+---------------------+-------------------------------+-----------------+\n",
            "| 41200 |                 0.0 |                           0.0 |           11.93 |\n",
            "+-------+---------------------+-------------------------------+-----------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76ab0b35",
        "outputId": "2a310c3b-4b4e-4a74-af16-f1a50fc9cb36"
      },
      "source": [
        "import os\n",
        "\n",
        "# % subscription_plan missing by country (potential MAR)\n",
        "query_mar = \"\"\"\n",
        "SELECT\n",
        "  country,\n",
        "  COUNT(*) AS n,\n",
        "  ROUND(100 * COUNTIF(subscription_plan IS NULL) / COUNT(*), 2) AS pct_missing_subscription_plan\n",
        "FROM\n",
        "  `{project_id}.netflix.users`\n",
        "GROUP BY\n",
        "  country\n",
        "ORDER BY\n",
        "  pct_missing_subscription_plan DESC;\n",
        "\"\"\".format(project_id=os.environ['GOOGLE_CLOUD_PROJECT'])\n",
        "\n",
        "# Save the query to a temporary file\n",
        "with open(\"mar_query.sql\", \"w\") as f:\n",
        "    f.write(query_mar)\n",
        "\n",
        "# Execute the query using bq query with the file\n",
        "!bq query --nouse_legacy_sql --quiet < mar_query.sql\n",
        "\n",
        "# Clean up the temporary file\n",
        "os.remove(\"mar_query.sql\")"
      ],
      "id": "76ab0b35",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------+-------------------------------+\n",
            "| country |   n   | pct_missing_subscription_plan |\n",
            "+---------+-------+-------------------------------+\n",
            "| Canada  | 12384 |                           0.0 |\n",
            "| USA     | 28816 |                           0.0 |\n",
            "+---------+-------+-------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpreting Missingness by Country (Potential MAR):**\n",
        "\n",
        "The query above shows the percentage of missing `subscription_plan` values for each `country`. If the percentage of missing values for `subscription_plan` varies significantly across different `country` values, this could be an indicator of **Missing At Random (MAR)**.\n",
        "\n",
        "MAR suggests that the probability of a value being missing is related to *other* observed variables in the dataset (in this case, `country`), but not to the value of the missing data itself. For example, if users in one country are significantly more likely to have a missing subscription plan than users in another country, the missingness is likely MAR and not MCAR (Missing Completely At Random).\n",
        "\n",
        "Understanding if data is MAR is important because it can influence how you handle missing values during data cleaning and feature engineering for machine learning models. Ignoring MAR can lead to biased analyses and models."
      ],
      "metadata": {
        "id": "5o0bOtcZ_YS4"
      },
      "id": "5o0bOtcZ_YS4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AV7owQInW6M3"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a query that prints the three missingness percentages from (1), rounded to two decimals.\n"
      ],
      "id": "AV7owQInW6M3"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Verify the three missingness percentages\n",
        "query = \"\"\"\n",
        "WITH base AS (\n",
        "  SELECT\n",
        "    COUNT(*) AS n,\n",
        "    COUNTIF(country IS NULL) AS miss_country,\n",
        "    COUNTIF(subscription_plan IS NULL) AS miss_plan,\n",
        "    COUNTIF(age IS NULL) AS miss_age\n",
        "  FROM\n",
        "    `{project_id}.netflix.users`\n",
        ")\n",
        "SELECT\n",
        "  ROUND(100 * miss_country / n, 2) AS pct_missing_country,\n",
        "  ROUND(100 * miss_plan / n, 2) AS pct_missing_subscription_plan,\n",
        "  ROUND(100 * miss_age / n, 2) AS pct_missing_age\n",
        "FROM\n",
        "  base;\n",
        "\"\"\".format(project_id=os.environ['GOOGLE_CLOUD_PROJECT'])\n",
        "\n",
        "# Save the query to a temporary file\n",
        "with open(\"verification_query.sql\", \"w\") as f:\n",
        "    f.write(query)\n",
        "\n",
        "# Execute the query using bq query with the file\n",
        "!bq query --nouse_legacy_sql --quiet < verification_query.sql\n",
        "\n",
        "# Clean up the temporary file\n",
        "os.remove(\"verification_query.sql\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xnMsiRe_qV8",
        "outputId": "fb51e4b4-c9fe-4f3d-f662-2314fcadf00a"
      },
      "id": "_xnMsiRe_qV8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------+-------------------------------+-----------------+\n",
            "| pct_missing_country | pct_missing_subscription_plan | pct_missing_age |\n",
            "+---------------------+-------------------------------+-----------------+\n",
            "|                 0.0 |                           0.0 |           11.93 |\n",
            "+---------------------+-------------------------------+-----------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2jUZCwuW6M3"
      },
      "source": [
        "**Reflection:** Which columns are most missing? Hypothesize MCAR/MAR/MNAR and why.\n",
        "\n",
        "\n",
        "Based on the outputs recieved, the age column has the highest precentage of missing values (11.93%).\n",
        "\n",
        "**MCAR (Missing Completely At Random)**: This would mean the missingness of age is completely random and not related to any other variables in the dataset (including age itself).\n",
        "\n",
        "**MAR (Missing At Random)**: This would mean the missingness of age is related to other observed variables in the dataset, but not to the age value itself.\n",
        "\n",
        "**MNAR (Missing Not At Random)**: This would mean the missingness of age is related to the unobserved age value itself.\n",
        "\n",
        "\n",
        "In order to fully hypothesize MCAR/MAR/MNAR, more analysis would be needed."
      ],
      "id": "B2jUZCwuW6M3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hER-KsGW6M3"
      },
      "source": [
        "### 5.2 Duplicates (watch_history) — What & Why\n",
        "Find exact duplicate interaction records and keep **one best** per group (deterministic policy)."
      ],
      "id": "2hER-KsGW6M3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AW4uC70mW6M3"
      },
      "source": [
        "### Build Prompt\n",
        "Generate **two BigQuery SQL cells**:\n",
        "1) Report duplicate groups on `(user_id, movie_id, event_ts, device_type)` with counts (top 20).\n",
        "2) Create table `watch_history_dedup` that keeps one row per group (prefer higher `progress_ratio`, then `minutes_watched`). Add comments.\n"
      ],
      "id": "AW4uC70mW6M3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUCtfId_W6M3"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — Detect duplicate groups (commented)\n",
        "# # SELECT user_id, movie_id, event_ts, device_type, COUNT(*) AS dup_count\n",
        "# # FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history`\n",
        "# # GROUP BY user_id, movie_id, event_ts, device_type\n",
        "# # HAVING dup_count > 1\n",
        "# # ORDER BY dup_count DESC\n",
        "# # LIMIT 20;"
      ],
      "id": "kUCtfId_W6M3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qdeY7gFW6M3"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — Keep-one policy (commented)\n",
        "# # CREATE OR REPLACE TABLE `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_dedup` AS\n",
        "# # SELECT * EXCEPT(rk) FROM (\n",
        "# #   SELECT h.*,\n",
        "# #          ROW_NUMBER() OVER (\n",
        "# #            PARTITION BY user_id, movie_id, event_ts, device_type\n",
        "# #            ORDER BY progress_ratio DESC, minutes_watched DESC\n",
        "# #          ) AS rk\n",
        "# #   FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history` h\n",
        "# # )\n",
        "# # WHERE rk = 1;"
      ],
      "id": "5qdeY7gFW6M3"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Detect duplicate groups on key columns and report top 20\n",
        "query_detect_duplicates = \"\"\"\n",
        "SELECT user_id, movie_id, watch_date, device_type, COUNT(*) AS dup_count\n",
        "FROM `{project_id}.netflix.watch_history`\n",
        "GROUP BY user_id, movie_id, watch_date, device_type\n",
        "HAVING dup_count > 1\n",
        "ORDER BY dup_count DESC\n",
        "LIMIT 20;\n",
        "\"\"\".format(project_id=os.environ['GOOGLE_CLOUD_PROJECT'])\n",
        "\n",
        "# Save the query to a temporary file\n",
        "with open(\"detect_duplicates_query.sql\", \"w\") as f:\n",
        "    f.write(query_detect_duplicates)\n",
        "\n",
        "# Execute the query using bq query with the file\n",
        "!bq query --nouse_legacy_sql --quiet < detect_duplicates_query.sql\n",
        "\n",
        "# Clean up the temporary file\n",
        "os.remove(\"detect_duplicates_query.sql\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSJ_1L8zFaoW",
        "outputId": "b3b92dc0-cc69-459b-c3a9-02422ed454a9"
      },
      "id": "nSJ_1L8zFaoW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+------------+------------+-------------+-----------+\n",
            "|  user_id   |  movie_id  | watch_date | device_type | dup_count |\n",
            "+------------+------------+------------+-------------+-----------+\n",
            "| user_03310 | movie_0640 | 2024-09-08 | Smart TV    |        16 |\n",
            "| user_00391 | movie_0893 | 2024-08-26 | Laptop      |        16 |\n",
            "| user_01292 | movie_0231 | 2024-07-05 | Laptop      |        12 |\n",
            "| user_03176 | movie_0534 | 2024-01-06 | Laptop      |        12 |\n",
            "| user_01807 | movie_0921 | 2025-01-30 | Laptop      |        12 |\n",
            "| user_03140 | movie_0205 | 2025-09-11 | Desktop     |        12 |\n",
            "| user_06799 | movie_0458 | 2024-08-15 | Desktop     |        12 |\n",
            "| user_01143 | movie_0166 | 2024-05-28 | Laptop      |        12 |\n",
            "| user_02976 | movie_0987 | 2024-09-19 | Desktop     |        12 |\n",
            "| user_03660 | movie_0109 | 2025-05-20 | Desktop     |        12 |\n",
            "| user_09973 | movie_0342 | 2025-03-22 | Desktop     |        12 |\n",
            "| user_08681 | movie_0332 | 2024-06-13 | Laptop      |        12 |\n",
            "| user_06462 | movie_0588 | 2025-02-10 | Laptop      |        12 |\n",
            "| user_00928 | movie_0913 | 2024-01-18 | Laptop      |        12 |\n",
            "| user_08276 | movie_0460 | 2024-04-24 | Desktop     |        12 |\n",
            "| user_01581 | movie_0933 | 2024-03-30 | Desktop     |        12 |\n",
            "| user_00472 | movie_0719 | 2024-12-04 | Laptop      |        12 |\n",
            "| user_02284 | movie_0914 | 2024-12-30 | Laptop      |        12 |\n",
            "| user_02126 | movie_0642 | 2025-02-09 | Desktop     |        12 |\n",
            "| user_05952 | movie_0893 | 2024-04-29 | Desktop     |        12 |\n",
            "+------------+------------+------------+-------------+-----------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Create a new table watch_history_dedup with one row per duplicate group\n",
        "# Policy: Prefer higher progress_percentage, then higher watch_duration_minutes\n",
        "query_dedup = \"\"\"\n",
        "CREATE OR REPLACE TABLE `{project_id}.netflix.watch_history_dedup` AS\n",
        "SELECT * EXCEPT(rk) FROM (\n",
        "  SELECT h.*,\n",
        "         ROW_NUMBER() OVER (\n",
        "           PARTITION BY user_id, movie_id, watch_date, device_type\n",
        "           ORDER BY progress_percentage DESC, watch_duration_minutes DESC\n",
        "         ) AS rk\n",
        "  FROM `{project_id}.netflix.watch_history` h\n",
        ")\n",
        "WHERE rk = 1;\n",
        "\"\"\".format(project_id=os.environ['GOOGLE_CLOUD_PROJECT'])\n",
        "\n",
        "# Save the query to a temporary file\n",
        "with open(\"dedup_query.sql\", \"w\") as f:\n",
        "    f.write(query_dedup)\n",
        "\n",
        "# Execute the query using bq query with the file\n",
        "!bq query --nouse_legacy_sql --quiet < dedup_query.sql\n",
        "\n",
        "# Clean up the temporary file\n",
        "os.remove(\"dedup_query.sql\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iMvGGs6FoB_",
        "outputId": "474082e1-42b6-478b-a5a3-09cef83f3883"
      },
      "id": "1iMvGGs6FoB_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Replaced mgmt-467-55510.netflix.watch_history_dedup\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0yO_qtlW6M5"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a before/after count query comparing raw vs `watch_history_dedup`.\n"
      ],
      "id": "l0yO_qtlW6M5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O01Z0rjPGNAj",
        "outputId": "37783597-8b97-48eb-94bc-c4b29af796b5"
      },
      "source": [
        "import os\n",
        "\n",
        "# Query to compare row counts before and after deduplication\n",
        "query = \"\"\"\n",
        "SELECT 'Original watch_history' AS table_name, COUNT(*) AS row_count FROM `{project_id}.netflix.watch_history`\n",
        "UNION ALL\n",
        "SELECT 'Deduplicated watch_history_dedup' AS table_name, COUNT(*) AS row_count FROM `{project_id}.netflix.watch_history_dedup`;\n",
        "\"\"\".format(project_id=os.environ['GOOGLE_CLOUD_PROJECT'])\n",
        "\n",
        "# Save the query to a temporary file\n",
        "with open(\"dedup_verification_query.sql\", \"w\") as f:\n",
        "    f.write(query)\n",
        "\n",
        "# Execute the query using bq query with the file\n",
        "!bq query --nouse_legacy_sql --quiet < dedup_verification_query.sql\n",
        "\n",
        "# Clean up the temporary file\n",
        "os.remove(\"dedup_verification_query.sql\")"
      ],
      "id": "O01Z0rjPGNAj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------+-----------+\n",
            "|            table_name            | row_count |\n",
            "+----------------------------------+-----------+\n",
            "| Original watch_history           |    420000 |\n",
            "| Deduplicated watch_history_dedup |    100000 |\n",
            "+----------------------------------+-----------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOIIGIg3W6M5"
      },
      "source": [
        "**Reflection:** Why do duplicates arise (natural vs system-generated)? How do they corrupt labels and KPIs?\n",
        "\n",
        "Duplicates can arise from many sources.\n",
        "\n",
        "Natural duplicates - Based on a user performing the same action multiple times (clicking something quickly repeatedly).\n",
        "\n",
        "System-generated - Occurs due to errors in the overall pipeline or process. Network issues often appear due to this.\n",
        "\n",
        "Duplicates can corrupt labels and KPIs through inflated counts of events, skewed distribution metrics, and misleading KPIs that are over or understated."
      ],
      "id": "rOIIGIg3W6M5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Um1MmjfwW6M5"
      },
      "source": [
        "### 5.3 Outliers (minutes_watched) — What & Why\n",
        "Estimate extreme values via IQR; report % outliers; **winsorize** to P01/P99 for robustness while also **flagging** extremes."
      ],
      "id": "Um1MmjfwW6M5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQw2BO17W6M5"
      },
      "source": [
        "### Build Prompt\n",
        "Generate **two BigQuery SQL cells**:\n",
        "1) Compute IQR bounds for `minutes_watched` on `watch_history_dedup` and report % outliers.\n",
        "2) Create `watch_history_robust` with `minutes_watched_capped` capped at P01/P99; return quantile summaries before/after.\n"
      ],
      "id": "rQw2BO17W6M5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u16Ke71nW6M5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "716dbb7b-6060-42a2-9563-d1b79df6fc80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------+--------------+\n",
            "| outliers | total  | pct_outliers |\n",
            "+----------+--------+--------------+\n",
            "|     3409 | 100000 |         3.41 |\n",
            "+----------+--------+--------------+\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Compute IQR bounds for watch_duration_minutes and report % outliers\n",
        "query_outliers = \"\"\"\n",
        "WITH dist AS (\n",
        "  SELECT\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 4)[OFFSET(1)] AS q1,\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 4)[OFFSET(3)] AS q3\n",
        "  FROM `{project_id}.netflix.watch_history_dedup`\n",
        "),\n",
        "bounds AS (\n",
        "  SELECT q1, q3, (q3-q1) AS iqr,\n",
        "         q1 - 1.5*(q3-q1) AS lo,\n",
        "         q3 + 1.5*(q3-q1) AS hi\n",
        "  FROM dist\n",
        ")\n",
        "SELECT\n",
        "  COUNTIF(h.watch_duration_minutes < b.lo OR h.watch_duration_minutes > b.hi) AS outliers,\n",
        "  COUNT(*) AS total,\n",
        "  ROUND(100*COUNTIF(h.watch_duration_minutes < b.lo OR h.watch_duration_minutes > b.hi)/COUNT(*),2) AS pct_outliers\n",
        "FROM `{project_id}.netflix.watch_history_dedup` h\n",
        "CROSS JOIN bounds b;\n",
        "\"\"\".format(project_id=os.environ['GOOGLE_CLOUD_PROJECT'])\n",
        "\n",
        "# Save the query to a temporary file\n",
        "with open(\"outliers_query.sql\", \"w\") as f:\n",
        "    f.write(query_outliers)\n",
        "\n",
        "# Execute the query using bq query with the file\n",
        "!bq query --nouse_legacy_sql --quiet < outliers_query.sql\n",
        "\n",
        "# Clean up the temporary file\n",
        "os.remove(\"outliers_query.sql\")"
      ],
      "id": "u16Ke71nW6M5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Verification Prompt\n",
        "Generate a query that shows min/median/max before vs after capping.\n"
      ],
      "metadata": {
        "id": "LU4T7p1hJGWA"
      },
      "id": "LU4T7p1hJGWA"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Create watch_history_robust with minutes_watched_capped capped at P01/P99\n",
        "# This step creates the table needed for the quantile summaries\n",
        "query_create_robust = \"\"\"\n",
        "CREATE OR REPLACE TABLE `{project_id}.netflix.watch_history_robust` AS\n",
        "WITH q AS (\n",
        "  SELECT\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 100)[OFFSET(1)]  AS p01,\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 100)[OFFSET(99)] AS p99\n",
        "  FROM `{project_id}.netflix.watch_history_dedup`\n",
        ")\n",
        "SELECT\n",
        "  h.*,\n",
        "  GREATEST(q.p01, LEAST(q.p99, h.watch_duration_minutes)) AS minutes_watched_capped\n",
        "FROM `{project_id}.netflix.watch_history_dedup` h, q;\n",
        "\"\"\".format(project_id=os.environ['GOOGLE_CLOUD_PROJECT'])\n",
        "\n",
        "# Save the query to a temporary file\n",
        "with open(\"create_robust_query.sql\", \"w\") as f:\n",
        "    f.write(query_create_robust)\n",
        "\n",
        "# Execute the query using bq query with the file\n",
        "!bq query --nouse_legacy_sql --quiet < create_robust_query.sql\n",
        "\n",
        "# Clean up the temporary file\n",
        "os.remove(\"create_robust_query.sql\")\n",
        "\n",
        "print(\"--- Quantile Summaries Before/After Capping ---\")\n",
        "\n",
        "# Query to show min/median/max before vs after capping\n",
        "query_quantile_summary = \"\"\"\n",
        "WITH\n",
        "  before_capping AS (\n",
        "    SELECT\n",
        "      'Before Capping' AS stage,\n",
        "      MIN(watch_duration_minutes) AS min_minutes,\n",
        "      APPROX_QUANTILES(watch_duration_minutes, 100)[OFFSET(50)] AS median_minutes,\n",
        "      MAX(watch_duration_minutes) AS max_minutes\n",
        "    FROM\n",
        "      `{project_id}.netflix.watch_history_dedup`\n",
        "  ),\n",
        "  after_capping AS (\n",
        "    SELECT\n",
        "      'After Capping' AS stage,\n",
        "      MIN(minutes_watched_capped) AS min_minutes,\n",
        "      APPROX_QUANTILES(minutes_watched_capped, 100)[OFFSET(50)] AS median_minutes,\n",
        "      MAX(minutes_watched_capped) AS max_minutes\n",
        "    FROM\n",
        "      `{project_id}.netflix.watch_history_robust`\n",
        "  )\n",
        "SELECT * FROM before_capping\n",
        "UNION ALL\n",
        "SELECT * FROM after_capping;\n",
        "\"\"\".format(project_id=os.environ['GOOGLE_CLOUD_PROJECT'])\n",
        "\n",
        "# Save the query to a temporary file\n",
        "with open(\"quantile_summary_query.sql\", \"w\") as f:\n",
        "    f.write(query_quantile_summary)\n",
        "\n",
        "# Execute the query using bq query with the file\n",
        "!bq query --nouse_legacy_sql --quiet < quantile_summary_query.sql\n",
        "\n",
        "# Clean up the temporary file\n",
        "os.remove(\"quantile_summary_query.sql\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTJENxKKIumU",
        "outputId": "fafe75c7-9974-4bbd-9261-8b01ccab99c8"
      },
      "id": "bTJENxKKIumU",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created mgmt-467-55510.netflix.watch_history_robust\n",
            "\n",
            "--- Quantile Summaries Before/After Capping ---\n",
            "+----------------+-------------+----------------+-------------+\n",
            "|     stage      | min_minutes | median_minutes | max_minutes |\n",
            "+----------------+-------------+----------------+-------------+\n",
            "| Before Capping |         0.2 |           51.2 |       799.3 |\n",
            "| After Capping  |         4.4 |           51.2 |       366.0 |\n",
            "+----------------+-------------+----------------+-------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZJB0x1HW6M6"
      },
      "source": [
        "**Reflection:** When might capping be harmful? Name a model type less sensitive to outliers and why.\n",
        "\n",
        "Capping is useful in handling extreme values to adjust data within a model. It can be harmful when the outliers are valuable to the data, the distribution is naturally heavy-tailed, and when you are aiming to interpret the impact of the original variable.\n",
        "\n",
        "A less sensitive type of model would be anything tree-based. These models partition data based on feature values and focus on rank order or relative differences rather than the magnitude. They do not rely on calculating means, variances, or distances the same way since they bin the data in a way that capping wouldn't impact the model as much.\n"
      ],
      "id": "WZJB0x1HW6M6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DVPz8KyW6M6"
      },
      "source": [
        "### 5.4 Business anomaly flags — What & Why\n",
        "Human-readable flags help both product decisioning and ML features (e.g., binge behavior)."
      ],
      "id": "0DVPz8KyW6M6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUEkaSpMW6M6"
      },
      "source": [
        "### Build Prompt\n",
        "Generate **three BigQuery SQL cells** (adjust if columns differ):\n",
        "1) In `watch_history_robust`, compute and summarize `flag_binge` for sessions > 8 hours.\n",
        "2) In `users`, compute and summarize `flag_age_extreme` if age can be parsed from `age_band` (<10 or >100).\n",
        "3) In `movies`, compute and summarize `flag_duration_anomaly` where `duration_min` < 15 or > 480 (if exists).\n",
        "Each cell should output count and percentage and include 1–2 comments.\n"
      ],
      "id": "xUEkaSpMW6M6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cab0d0c",
        "outputId": "b24211f2-99c0-4006-882c-b49cd7538594"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import os\n",
        "PROJECT_ID = input(\"Enter your GCP Project ID: \").strip()\n",
        "REGION = \"us-central1\"  # keep consistent; change if instructed\n",
        "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
        "os.environ[\"REGION\"] = REGION # Export the REGION environment variable\n",
        "print(\"Project:\", PROJECT_ID, \"| Region:\", REGION)\n",
        "\n",
        "# Set active project for gcloud/BigQuery CLI\n",
        "!gcloud config set project $GOOGLE_CLOUD_PROJECT\n",
        "!gcloud config get-value project\n",
        "# Done: Auth + Project/Region set"
      ],
      "id": "0cab0d0c",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your GCP Project ID: mgmt-467-55510\n",
            "Project: mgmt-467-55510 | Region: us-central1\n",
            "Updated property [core/project].\n",
            "mgmt-467-55510\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 1. Compute and summarize flag_binge for sessions > 8 hours in watch_history_robust\n",
        "# This flag identifies potentially long viewing sessions.\n",
        "query_binge = \"\"\"\n",
        "SELECT\n",
        "  COUNTIF(minutes_watched_capped > 8*60) AS sessions_over_8h,\n",
        "  COUNT(*) AS total,\n",
        "  ROUND(100*COUNTIF(minutes_watched_capped > 8*60)/COUNT(*),2) AS pct_sessions_over_8h\n",
        "FROM `{project_id}.netflix.watch_history_robust`;\n",
        "\"\"\".format(project_id=os.environ['GOOGLE_CLOUD_PROJECT'])\n",
        "\n",
        "# Save the query to a temporary file\n",
        "with open(\"binge_query.sql\", \"w\") as f:\n",
        "    f.write(query_binge)\n",
        "\n",
        "# Execute the query using bq query with the file\n",
        "!bq query --nouse_legacy_sql --quiet < binge_query.sql\n",
        "\n",
        "# Clean up the temporary file\n",
        "os.remove(\"binge_query.sql\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vhy96uQH9Nq",
        "outputId": "2de46328-3904-4a0c-e111-618e005c86a6"
      },
      "id": "_vhy96uQH9Nq",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+--------+----------------------+\n",
            "| sessions_over_8h | total  | pct_sessions_over_8h |\n",
            "+------------------+--------+----------------------+\n",
            "|                0 | 100000 |                  0.0 |\n",
            "+------------------+--------+----------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 2. Compute and summarize flag_age_extreme if age is <10 or >100 in the users table\n",
        "# This flag identifies users with potentially extreme age values.\n",
        "query_age_extreme = \"\"\"\n",
        "SELECT\n",
        "  COUNTIF(age < 10 OR age > 100) AS extreme_age_rows,\n",
        "  COUNT(*) AS total,\n",
        "  ROUND(100*COUNTIF(age < 10 OR age > 100)/COUNT(*),2) AS pct_extreme_age\n",
        "FROM `{project_id}.netflix.users`\n",
        "WHERE age IS NOT NULL; -- Only consider rows where age is not missing\n",
        "\"\"\".format(project_id=os.environ['GOOGLE_CLOUD_PROJECT'])\n",
        "\n",
        "# Save the query to a temporary file\n",
        "with open(\"age_extreme_query.sql\", \"w\") as f:\n",
        "    f.write(query_age_extreme)\n",
        "\n",
        "# Execute the query using bq query with the file\n",
        "!bq query --nouse_legacy_sql --quiet < age_extreme_query.sql\n",
        "\n",
        "# Clean up the temporary file\n",
        "os.remove(\"age_extreme_query.sql\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivlBujquv-29",
        "outputId": "fed5d58f-edee-4406-e9f3-9ccf1584fc0d"
      },
      "id": "ivlBujquv-29",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+-------+-----------------+\n",
            "| extreme_age_rows | total | pct_extreme_age |\n",
            "+------------------+-------+-----------------+\n",
            "|              716 | 36284 |            1.97 |\n",
            "+------------------+-------+-----------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 3. Compute and summarize flag_duration_anomaly where duration_minutes < 15 or > 480 in the movies table\n",
        "# This flag identifies movies with potentially anomalous durations.\n",
        "query_duration_anomaly = \"\"\"\n",
        "SELECT\n",
        "  COUNTIF(duration_minutes < 15 OR duration_minutes > 480) AS duration_anomaly_rows,\n",
        "  COUNT(*) AS total,\n",
        "  ROUND(100*COUNTIF(duration_minutes < 15 OR duration_minutes > 480)/COUNT(*),2) AS pct_duration_anomaly\n",
        "FROM `{project_id}.netflix.movies`\n",
        "WHERE duration_minutes IS NOT NULL; -- Only consider rows where duration_minutes is not missing\n",
        "\"\"\".format(project_id=os.environ['GOOGLE_CLOUD_PROJECT'])\n",
        "\n",
        "# Save the query to a temporary file\n",
        "with open(\"duration_anomaly_query.sql\", \"w\") as f:\n",
        "    f.write(query_duration_anomaly)\n",
        "\n",
        "# Execute the query using bq query with the file\n",
        "!bq query --nouse_legacy_sql --quiet < duration_anomaly_query.sql\n",
        "\n",
        "# Clean up the temporary file\n",
        "os.remove(\"duration_anomaly_query.sql\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqwMRiRbwImB",
        "outputId": "636cd059-1e07-43a0-be19-fb5c81e96037"
      },
      "id": "fqwMRiRbwImB",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------+-------+----------------------+\n",
            "| duration_anomaly_rows | total | pct_duration_anomaly |\n",
            "+-----------------------+-------+----------------------+\n",
            "|                    92 |  4160 |                 2.21 |\n",
            "+-----------------------+-------+----------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2djFf5zlW6M8"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a single compact summary query that returns two columns per flag: `flag_name, pct_of_rows`.\n"
      ],
      "id": "2djFf5zlW6M8"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Generate a single compact summary query that returns two columns per flag: flag_name, pct_of_rows\n",
        "query_summary = \"\"\"\n",
        "SELECT 'flag_binge' AS flag_name, ROUND(100*COUNTIF(minutes_watched_capped > 8*60)/COUNT(*),2) AS pct_of_rows FROM `{project_id}.netflix.watch_history_robust`\n",
        "UNION ALL\n",
        "SELECT 'flag_age_extreme' AS flag_name, ROUND(100*COUNTIF(age < 10 OR age > 100)/COUNT(*),2) AS pct_of_rows FROM `{project_id}.netflix.users` WHERE age IS NOT NULL\n",
        "UNION ALL\n",
        "SELECT 'flag_duration_anomaly' AS flag_name, ROUND(100*COUNTIF(duration_minutes < 15 OR duration_minutes > 480)/COUNT(*),2) AS pct_of_rows FROM `{project_id}.netflix.movies` WHERE duration_minutes IS NOT NULL;\n",
        "\"\"\".format(project_id=os.environ['GOOGLE_CLOUD_PROJECT'])\n",
        "\n",
        "# Save the query to a temporary file\n",
        "with open(\"anomaly_summary_query.sql\", \"w\") as f:\n",
        "    f.write(query_summary)\n",
        "\n",
        "# Execute the query using bq query with the file\n",
        "!bq query --nouse_legacy_sql --quiet < anomaly_summary_query.sql\n",
        "\n",
        "# Clean up the temporary file\n",
        "os.remove(\"anomaly_summary_query.sql\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7HpH7j31Mdw",
        "outputId": "8fbd6ae7-151d-40f1-c622-90ad049fb738"
      },
      "id": "E7HpH7j31Mdw",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------+-------------+\n",
            "|       flag_name       | pct_of_rows |\n",
            "+-----------------------+-------------+\n",
            "| flag_binge            |         0.0 |\n",
            "| flag_age_extreme      |        1.97 |\n",
            "| flag_duration_anomaly |        2.21 |\n",
            "+-----------------------+-------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcMolu56W6M8"
      },
      "source": [
        "**Reflection:** Which anomaly flag is most common? Which would you keep as a feature and why?\n",
        "\n",
        "Based on the summary query output:\n",
        "\n",
        "*   `flag_binge`: 0.0%\n",
        "*   `flag_age_extreme`: 1.97%\n",
        "*   `flag_duration_anomaly`: 2.21%\n",
        "\n",
        "The most common anomaly flag is flag_duration_anomaly, indicating that a small percentage of movies have durations that fall outside the typical range (less than 15 minutes or more than 8 hours).\n",
        "\n",
        "Which flag to keep as a feature depends on the specific business problem or machine learning task. However, `flag_age_extreme` and `flag_duration_anomaly` seem potentially more directly useful as features for many tasks compared to `flag_binge` (which is 0% in this dataset).\n",
        "\n",
        "*   **`flag_age_extreme`:** This could be a useful feature for understanding user behavior or targeting. Users with extreme ages might have different viewing habits or content preferences. It could also be an indicator of potential data entry errors if the extreme ages are not plausible.\n",
        "*   **`flag_duration_anomaly`:** This could be a valuable feature for recommendation systems or content analysis. Movies with unusually short or long durations might appeal to different audiences or indicate different content types (e.g., short films, documentaries).\n",
        "\n",
        "\n",
        "Likely keep **`flag_age_extreme`** and **`flag_duration_anomaly`** as features, as they identify potentially distinct groups of users and content that could influence various downstream tasks.\n"
      ],
      "id": "LcMolu56W6M8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqzB6se0W6M8"
      },
      "source": [
        "## 6) Save & submit — What & Why\n",
        "Reproducibility: save artifacts and document decisions so others can rerun and audit."
      ],
      "id": "gqzB6se0W6M8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I05LH9knW6M8"
      },
      "source": [
        "### Build Prompt\n",
        "Generate a checklist (Markdown) students can paste at the end:\n",
        "- Save this notebook to the team Drive.\n",
        "- Export a `.sql` file with your DQ queries and save to repo.\n",
        "- Push notebook + SQL to the **team GitHub** with a descriptive commit.\n",
        "- Add a README with your `PROJECT_ID`, `REGION`, bucket, dataset, and today’s row counts.\n"
      ],
      "id": "I05LH9knW6M8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a checklist for saving and submitting your work:\n",
        "\n",
        "- [ ] Save this notebook to your team's shared Google Drive folder. Make sure the filename is clear and includes your team name or initials.\n",
        "- [ ] Export your Data Quality (DQ) SQL queries into a single `.sql` file. You can copy the relevant SQL statements from the code cells in sections 5.1, 5.2, 5.3, and 5.4.\n",
        "- [ ] Save the `.sql` file in your local repository.\n",
        "- [ ] Push both the notebook (`.ipynb` file) and the `.sql` file to your team's GitHub repository. Write a descriptive commit message summarizing the work done (e.g., \"Completed DQ profiling and cleaning for Netflix dataset\").\n",
        "- [ ] Add or update your team's README file in the GitHub repository. Include the following information:\n",
        "    - Your GCP `PROJECT_ID`.\n",
        "    - The `REGION` used for your resources.\n",
        "    - The name of your GCS bucket (`BUCKET_NAME`).\n",
        "    - The name of your BigQuery dataset (`DATASET`).\n",
        "    - The row counts for all six tables in the `netflix` dataset obtained from the verification step in section 4.\n",
        "    - A brief summary of the data quality issues found (missingness, duplicates, outliers, anomalies) and how you addressed them."
      ],
      "metadata": {
        "id": "rlteB7Py1kT2"
      },
      "id": "rlteB7Py1kT2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcQoQ6S3W6M9"
      },
      "source": [
        "## Grading rubric (quick)\n",
        "- Profiling completeness (30)  \n",
        "- Cleaning policy correctness & reproducibility (40)  \n",
        "- Reflection/insight (20)  \n",
        "- Hygiene (naming, verification, idempotence) (10)\n"
      ],
      "id": "IcQoQ6S3W6M9"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}